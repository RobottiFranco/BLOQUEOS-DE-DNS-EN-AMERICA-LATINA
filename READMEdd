# BLOQUEOS-DE-DNS-EN-AMERICA-LATINA

## Flujo de obtencion primaria de datos

La clase main de este flujo es "fetch_db_data.py" en especifico la funcion de "extraer_datos_historicos". lo que realiza esta funcion es dado un aÃ±o inicio y fin recorrer todos los paises interes y realizar una llamada al servicio de "get_global_list" con la query especifica, la cota superior de filas que me puede traer esa request, el directorio a utiliza, el nombre del arhivo a guardar, el modo de escritura y una flag para la remocion de duplicados y filtrado.

```python
countries: list = ["UY", "VE", "HN", "AR", "CU", "SV", "NI", "GT"]
query_db = "https://api.ooni.org/api/v1/measurements"

def extraer_datos_historicos(since: int = 2023, until: int = 2025) -> None:
    for country in countries:
        while since <= until:
            for month in range(1, 13):
                if since == 2025 and month == 2:
                    break
                inicio, final = get_month_range(since, month)
                query_builder = QueryBuilder() \
                    .set_base_url(query_db) \
                    .set_test_name("web_connectivity") \
                    .set_probe_cc(country) \
                    .set_since(inicio) \
                    .set_until(final) \
                    .build()

                get_global_list(query_builder, 2000, True, "src/data/raw", "lista_global_bruta", "a", True)
            since += 1
        since = 2023
```

lo que realiza el servicio es la preparacion de la la query, la obtencion de los datos, el filtrado y la escritura de los mismos en un archivo csv.

```python
def filter_data(data, update: bool):
    if update == True:
        filtered_data = filter_and_remove_duplicates(data)
    else:
        filtered_data = filter_dns(data)
    return filtered_data


def get_global_list(query: Query, limit: int, anomaly: bool, output_directory: str, file_name: str, mode: str, update: bool) -> None:
    try:
        print(f"Iniciando el proceso de datos para {query.probe_cc}... desde {query.since} hasta {query.until}")
        query_db = QueryDB(query, limit, anomaly)
        query_db = query_db.query_db()
        data = fetch_data(query_db)
        if not data:
            print(f"No se pudieron obtener datos de {query.probe_cc} desde {query.since} hasta {query.until}")
            return
        
        filtered_data = filter_data(data, update)

        path = create_file_and_path(output_directory, f"{file_name}.csv")
        save_to_csv(path, filtered_data, mode)
    except Exception as e:
        print(f"Error al obtener datos: {e}")
```

## Flujo de obtencion secundaria de datos

El archivo main es el mismo que en el flujo anterior, pero en este caso lo que se realiza es un setteo de los dias que se ejecutaron las listas, el id de la lista. ademas de modificar el numero maximo de filas, el nombre del archivo a guardar y la flag de remocion de duplicados y filtrado.

```python
def extraer_datos_actualizados() -> None:
    if os.path.exists("src/data/raw/lista_global_actualizada.csv"):
        os.remove("src/data/raw/lista_global_actualizada.csv")
    since = 1
    until = 16
    while since <= until:
        query_builder = QueryBuilder() \
            .set_base_url(query_db) \
            .set_test_name("web_connectivity") \
            .set_since(f"2025-01-{since}") \
            .set_until(f"2025-01-{since + 1}") \
            .set_ooni_run_link_id("10118") \
            .build()
        get_global_list(query_builder, 4000, True, "src/data/raw", "lista_global_actualizada", "a", False)
        since += 1
```

## Flujo de procesamiento de datos

La clase main de este flujo es "fetch_db_data.py". esta clase tiene una funcion que es en base al archivo entrada, el nombre del archivo de salida y el directorio de salida, creando una query de tipo raw y por cada fila en el archivo de entrada se carga el id de la medicion y por cada medicion llama al serivico get_lock_type.

```python
def lock_type(archivo_entrada: str, output_directory: str):
    measurement_uids = []
    query = QueryBuilder() \
        .set_base_url("https://api.ooni.org/api/v1/raw_measurement") \
        .build()
        
    with open(archivo_entrada, mode='r') as file:
        reader = csv.DictReader(file)
        for row in reader:
            measurement_uids.append(row.get('measurement_uid', None))

    for measurement_uid in measurement_uids:
        get_lock_type(query, measurement_uid, output_directory, mode='a')
        
lock_type("src/data/raw/lista_global_actualizada.csv", "src/data/comparative")
```

lo que realiza el servicio es la preparacion de la la query, la obtencion de los datos, y el filtrado por tipo de datos de interes, en nuestro caso: "measurement_uid", "input", "dns_experiment_failure", "http_experiment_failure", "dns_consistency", "accessible", "resolver_asn", "resolver_ip", "status_code", "tcp_ip", "tcp_status".
estos valores los guarda en un archivo csv separandolos por resolvedor ip.

```python
def get_lock_type(query: Query, measurement_uid: str, output_directory: str, mode: str) -> None:
    try:
        query_raw = QueryRaw(query)
        query_raw = query_raw.QueryRaw(measurement_uid)
        data = fetch_data(query_raw)
        if not data:
            print(f"No se pudieron obtener datos")
            return
        
        test_keys = data.get("test_keys", {})
        control = test_keys.get("control", {})
        http_request = control.get("http_request", {})

        tcp_connect = test_keys.get("tcp_connect", [])
        tcp_connect_info = None
        if tcp_connect:
            tcp_connect_info = tcp_connect[0] 

        row = {
            "measurement_uid": measurement_uid or "None",
            "input": data.get("input", "None"),
            "dns_experiment_failure": test_keys.get("dns_experiment_failure", "None")  or "None",
            "http_experiment_failure": test_keys.get("http_experiment_failure", "None")  or "None",
            "dns_consistency": test_keys.get("dns_consistency", "None")  or "None",
            "accessible": test_keys.get("accessible", "None"),
            "resolver_asn": data.get("resolver_asn", "None"),
            "resolver_ip": data.get("resolver_ip", "None"),
            "status_code": http_request.get("status_code", "None"),
            "tcp_ip": tcp_connect_info.get("ip", "None") if tcp_connect_info else "None",
            "tcp_status": tcp_connect_info.get("status", "None") if tcp_connect_info else "None"
        }
        
        for key, value in row.items():
            """  quiero que por cada valor diferente en la linea de resolver_ip escriba en un archivo diferente"""
            if key == "resolver_ip":
                path = create_file_and_path(output_directory, f"{value}.csv")
                escribir_tipo_de_bloqueo_csv(path, [row], mode)
        
    except Exception as e:
        print(f"Error al obtener datos: {e}")
```

## Flujo de eliminacion de paginas no existentes

La clase main de este flujo es "mesh_system.py". esta clase tiene una funcion que es en base al directorio de entrada (donde de guardan todos los archivos del flujo anterior),
se llama al serivicio de "process_files"

```python
def mesh(directory: str) -> None:
    output_file = process_files(directory)
    print(f"Archivo generado con archivos inexistentes: {output_file}")

mesh("src/data/comparative")
```

el serivicio lo que realiza es la lectura de todos los archivos en el directorio de entrada, y por cada archivo lee todas las filas y por cada fila lee el valor de "input", toma un archivo como pivote y compara con el resto de archivos, si el valor se repite en todos los archivos se coloca en el archivo de salida "pages_no_longer_exist.csv", esto ocurre por cada uno de los archivos tomandose como pivote.

```python
def process_files(directory):
    csv_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]
    dataframes = {file: pd.read_csv(file) for file in csv_files}

    all_common_inputs = pd.DataFrame(columns=["input"])

    for file in csv_files:
        reference_data = dataframes[file]
        common_inputs = compare_files(reference_data, dataframes, file)
        all_common_inputs = pd.concat([all_common_inputs, common_inputs]).drop_duplicates().reset_index(drop=True)

    output_file = create_file_and_path(directory, "pages_no_longer_exist.csv")
    all_common_inputs.to_csv(output_file, index=False)

    return output_file
```

